{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.1f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data (numpy arrays)\n",
    "\n",
    "path_train = r'C:\\Users\\aldan\\OneDrive\\Escritorio\\Study Notes\\0 - Data Sets\\Img_array_dataset\\Primary categories - Train.npz'\n",
    "path_val = r'C:\\Users\\aldan\\OneDrive\\Escritorio\\Study Notes\\0 - Data Sets\\Img_array_dataset\\Primary categories - Validation.npz'\n",
    "path_test = r'C:\\Users\\aldan\\OneDrive\\Escritorio\\Study Notes\\0 - Data Sets\\Img_array_dataset\\Primary categories - Test.npz'\n",
    "\n",
    "data_train = np.load(path_train)\n",
    "data_val = np.load(path_val)\n",
    "data_test = np.load(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'labels'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'labels'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'labels'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Displaying the data we loaded\n",
    "display(\n",
    "    dict(data_train).keys()\n",
    "    , dict(data_val).keys()\n",
    "    , dict(data_test).keys()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12963, 120, 90, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking it's shape\n",
    "data_train['images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), (12963,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking unique values\n",
    "np.unique(data_train['labels']), data_train['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the data for train, validation, test \n",
    "images_train = data_train['images']\n",
    "labels_train = data_train['labels']\n",
    "\n",
    "images_val = data_val['images']\n",
    "labels_val = data_val['labels']\n",
    "\n",
    "images_test = data_test['images']\n",
    "labels_test = data_test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the values for optimization purposes\n",
    "images_train = images_train/255.0\n",
    "images_val = images_val/255.0\n",
    "images_test = images_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the variables and hyperparameters\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "HP_FILTER_SIZES = hp.HParam('filter_size', hp.Discrete([3,5,7]))\n",
    "HP_FILTER_NUM = hp.HParam('filter_number', hp.Discrete([32,64,96,128]))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer(r'logs/Model_1/hparam_tuning/').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_FILTER_SIZES, HP_FILTER_NUM],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams, session_num):\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(filters = hparams[HP_FILTER_NUM], kernel_size = hparams[HP_FILTER_SIZES], activation = 'relu', input_shape = (120, 90, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(filters = hparams[HP_FILTER_NUM], kernel_size = 3, activation = 'relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(3)\n",
    "    ])\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss = loss_fn, metrics = ['accuracy'])\n",
    "\n",
    "    log_dir = r'logs/Model_1/fit/run-{}'.format(session_num)\n",
    "\n",
    "    def plot_confusion_matrix(cm, class_names):\n",
    "        \"\"\"\n",
    "        Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "        Args:\n",
    "          cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "          class_names (array, shape = [n]): String names of the integer classes\n",
    "        \"\"\"\n",
    "        figure = plt.figure(figsize=(12, 12))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title(\"Confusion matrix\")\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names, rotation=45)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "\n",
    "        # Normalize the confusion matrix.\n",
    "        cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "        # Use white text if squares are dark; otherwise black.\n",
    "        threshold = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "            plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        return figure   \n",
    "    \n",
    "    def plot_to_image(figure):\n",
    "        \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "        returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "        # Save the plot to a PNG in memory.\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        # Closing the figure prevents it from being displayed directly inside\n",
    "        # the notebook.\n",
    "        plt.close(figure)\n",
    "        buf.seek(0)\n",
    "        # Convert PNG buffer to TF image\n",
    "        image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "        # Add the batch dimension\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        return image\n",
    "    \n",
    "    # Defining a file writer for Confusion Matrix logging purposes\n",
    "    file_writer_cm = tf.summary.create_file_writer(log_dir + '/cm')     \n",
    "    \n",
    "    def log_confusion_matrix(epoch, logs):\n",
    "        \"\"\"\n",
    "        Saves the confusion matrix event \n",
    "\n",
    "        Args:\n",
    "            epoch: number of epochs\n",
    "            logs: directory to save the logs\n",
    "        \"\"\"\n",
    "        # Use the model to predict the values from the validation dataset.\n",
    "        test_pred_raw = model.predict(images_val)\n",
    "        test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "\n",
    "        # Calculate the confusion matrix.\n",
    "        cm = sklearn.metrics.confusion_matrix(labels_val, test_pred)\n",
    "        # Log the confusion matrix as an image summary.\n",
    "        figure = plot_confusion_matrix(cm, class_names=['Glasses/Sunglasses', 'Trousers/Jeans', 'Shoes'])\n",
    "        cm_image = plot_to_image(figure)\n",
    "\n",
    "        # Log the confusion matrix as an image summary.\n",
    "        with file_writer_cm.as_default():\n",
    "            tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\n",
    "    \n",
    "    # Define the Tensorboard and Confusion Matrix callbacks.\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)\n",
    "    cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
    "\n",
    "    # Defining early stopping to prevent overfitting\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        mode = 'auto',\n",
    "        min_delta = 0,\n",
    "        patience = 2,\n",
    "        verbose = 0, \n",
    "        restore_best_weights = True\n",
    "    )\n",
    "    \n",
    "    # Training the model\n",
    "    model.fit(\n",
    "        images_train,\n",
    "        labels_train,\n",
    "        epochs = EPOCHS,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        callbacks = [tensorboard_callback, cm_callback, early_stopping],\n",
    "        validation_data = (images_val,labels_val),\n",
    "        verbose = 2\n",
    "    )\n",
    "     \n",
    "    # Evaluating the model's performance on the validation set\n",
    "    _, accuracy = model.evaluate(images_val,labels_val)\n",
    "    \n",
    "    # Saving the current model for future reference\n",
    "    model.save(r\"saved_models\\Model_1\\Run-{}\".format(session_num))\n",
    "    \n",
    "    return accuracy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to train the model and log the results\n",
    "def run(log_dir, hparams, session_num):\n",
    "    \n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        # record the values used in this trial\n",
    "        hp.hparams(hparams)  \n",
    "        accuracy = train_test_model(hparams, session_num)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-1\n",
      "{'filter_size': 5, 'filter_number': 64}\n",
      "Epoch 1/15\n",
      "51/51 [==============================] - 4s 84ms/step\n",
      "203/203 - 163s - loss: 0.0888 - accuracy: 0.9784 - val_loss: 0.0144 - val_accuracy: 0.9975 - 163s/epoch - 805ms/step\n",
      "Epoch 2/15\n",
      "51/51 [==============================] - 4s 69ms/step\n",
      "203/203 - 148s - loss: 0.0185 - accuracy: 0.9965 - val_loss: 0.0155 - val_accuracy: 0.9981 - 148s/epoch - 730ms/step\n",
      "Epoch 3/15\n",
      "51/51 [==============================] - 3s 67ms/step\n",
      "203/203 - 125s - loss: 0.0156 - accuracy: 0.9976 - val_loss: 0.0070 - val_accuracy: 0.9988 - 125s/epoch - 617ms/step\n",
      "Epoch 4/15\n",
      "51/51 [==============================] - 3s 67ms/step\n",
      "203/203 - 125s - loss: 0.0078 - accuracy: 0.9991 - val_loss: 0.0055 - val_accuracy: 0.9988 - 125s/epoch - 618ms/step\n",
      "Epoch 5/15\n",
      "51/51 [==============================] - 4s 69ms/step\n",
      "203/203 - 126s - loss: 0.0048 - accuracy: 0.9995 - val_loss: 0.0052 - val_accuracy: 0.9988 - 126s/epoch - 619ms/step\n",
      "Epoch 6/15\n",
      "51/51 [==============================] - 3s 64ms/step\n",
      "203/203 - 132s - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0252 - val_accuracy: 0.9944 - 132s/epoch - 651ms/step\n",
      "Epoch 7/15\n",
      "51/51 [==============================] - 3s 63ms/step\n",
      "203/203 - 129s - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0096 - val_accuracy: 0.9988 - 129s/epoch - 636ms/step\n",
      "51/51 [==============================] - 4s 70ms/step - loss: 0.0052 - accuracy: 0.9988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_2\\Run-1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_2\\Run-1\\assets\n"
     ]
    }
   ],
   "source": [
    "# For loop to run and train the model using the different hyperparameters\n",
    "session_num = 1\n",
    "\n",
    "# Set the for nested loop for all the combinations between hyperparameters\n",
    "for filter_size in HP_FILTER_SIZES.domain.values:\n",
    "    for filter_num in HP_FILTER_NUM.domain.values:\n",
    "\n",
    "        hparams = {\n",
    "            HP_FILTER_SIZES: filter_size,\n",
    "            HP_FILTER_NUM: filter_num\n",
    "        }\n",
    "\n",
    "        run_name = \"run-%d\" % session_num\n",
    "        # print useful information\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "        # train and export the model\n",
    "        run('Logs/Model_1/hparam_tuning/' + run_name, hparams, session_num)\n",
    "\n",
    "        session_num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('Deep Reinforcement Learning': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51a29eaf9359bf8bbba3bbd4aa5e04cbad8539c83717ab508faa867bced1b80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
